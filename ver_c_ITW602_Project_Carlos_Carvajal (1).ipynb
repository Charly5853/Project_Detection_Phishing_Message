{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b612ea6b-e917-40fd-9d7f-2722e2fc1690",
   "metadata": {},
   "source": [
    "# Title: Use Machine Learning and Python to Develop Software, for Detect Financial Fraud in Text Messages \n",
    "Subject: ITW602 Advanced Information Technology - Work Integrated  \n",
    "\n",
    "Group: 14\n",
    "Student:  Carlos Carvajal\n",
    "Id A00133955\n",
    "Master of Software Engineering\n",
    "\n",
    "Professor:\n",
    "PhD Amr van den Adel\n",
    "\n",
    "Torrens Univerity\n",
    "Australia\n",
    "Trimester 3 HE, 2025 (September 15 - December 7)\n",
    "Date: dd-mm-yyyy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d82cd8-78f7-4e9b-9916-e07296414d8f",
   "metadata": {},
   "source": [
    "# 1 Business Understanding (Introductions)\n",
    "\n",
    "1. Business Understanding (Introductions)\n",
    "The problem context, objectives, scope, etc. are defined.\n",
    "2. Data Understanding\n",
    "The database is accessed (Chapman et al., 2000; kaggle, n.d.), the data type and content are analyzed, the data is checked for data issues, and the data is visualized.\n",
    "3. Data Preparation\n",
    "The data is prepared for processing, labels are converted into categorical data, stopwords are removed, and the data is divided into tokenization, stemming, and lemmatization. Finally, the data is prepared for training and testing.\n",
    "4. Modeling\n",
    "The N-Gram mathematical models are integrated with Python.\n",
    "5. Evaluation\n",
    "A qualitative and quantitative analysis of performance, as well as the precision and accuracy of the results, is performed using the confusion matrix.\n",
    "6. Deployment\n",
    "A test is performed using the Python shell (prototype), with a real-life example, and the results are analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9203b-64ed-4ce1-9443-d44e053b767f",
   "metadata": {},
   "source": [
    "# 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341cb40-bb4e-42ca-aec7-ab3f7e78a684",
   "metadata": {},
   "source": [
    "## Import Pythons 's Libraries Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70886300-8b31-42aa-9367-8a8be83f0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Run Ok 2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# phishing_pipeline.py  (run in your environment)\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "# ML tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print(' Test Run Ok 2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f2f5b-3b75-4328-a412-ada2973fdeb7",
   "metadata": {},
   "source": [
    "Load the Dataset into a Pandas DataFrame\n",
    "Access the page where the data is located, download the computer, and connect to Python to analyze its consistency, verify the data type, and repair errors.\n",
    "Upload file.csv from local computer\n",
    "Datase Reference: Salem, A. (2025). Spam_email_prediction. Kaggle - Dataset. https://www.kaggle.com/code/ayahsalemalshdefat/spam-email-prediction/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c8f625-4451-4c3a-b09b-2e4dc13173aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['text_combined', 'label']\n",
      " Test 2.2 Run OK\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Config / paths\n",
    "# -------------------------\n",
    "DATA_PATH = \"./data/phishing_emails.csv\"   # <- change to actual file from Kaggle\n",
    "MODEL_OUT = \"./best_phishing_model.joblib\"\n",
    "VECT_OUT = \"./tfidf_vectorizer.joblib\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load dataset\n",
    "# -------------------------\n",
    "DATA_PATH = \"phishing_email.csv\"\n",
    "#df = pd.read_csv('email.csv', encoding= \"ISO-8859-1\" )\n",
    "df = pd.read_csv(DATA_PATH, encoding='utf-8', low_memory=False)\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "# find columns with text/labels, adjust below:\n",
    "# — assume label column might be named 'label' or 'Category' or 'class'\n",
    "# Detect columns\n",
    "label_col_candidates = ['label', 'Label', 'Category', 'category', 'class', 'target']\n",
    "text_col_candidates  = ['text', 'message', 'Message', 'email', 'body', 'content', 'text_combined']\n",
    "\n",
    "label_col = next((c for c in label_col_candidates if c in df.columns), None)\n",
    "text_col  = next((c for c in text_col_candidates  if c in df.columns), None)\n",
    "\n",
    "if not label_col or not text_col:\n",
    "    raise RuntimeError(f\"Could not find label/text column. Found: label={label_col}, text={text_col}\")\n",
    "\n",
    "# Keep and rename\n",
    "df = df[[text_col, label_col]].rename(columns={\n",
    "    text_col: 'text',\n",
    "    label_col: 'label'\n",
    "}).copy()\n",
    "print(' Test 2.2 Run OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca24a6-9110-4560-8698-0234db248d2e",
   "metadata": {},
   "source": [
    "## Check dataset file.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17853b86-7b4a-4cec-99d6-4af9c063a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "\n",
      "head\n",
      "                                       text_combined  label\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...      0\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...      0\n",
      "2  enron actuals march 30 april 1 201 estimated a...      0\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...      0\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...      0\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "columns names\n",
      "Index(['text_combined', 'label'], dtype='object')\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "Count of columns : 2\n",
      "Count of rows :   82486\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "infor\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82486 entries, 0 to 82485\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   text_combined  82486 non-null  object\n",
      " 1   label          82486 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      " tail \n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "shape\n",
      "(82486, 2)\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "length of data is 82486\n",
      "types\n",
      "text_combined    object\n",
      "label             int64\n",
      "dtype: object\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "describe\n",
      "              label\n",
      "count  82486.000000\n",
      "mean       0.519979\n",
      "std        0.499604\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "isnull()\n",
      "       text_combined  label\n",
      "0              False  False\n",
      "1              False  False\n",
      "2              False  False\n",
      "3              False  False\n",
      "4              False  False\n",
      "...              ...    ...\n",
      "82481          False  False\n",
      "82482          False  False\n",
      "82483          False  False\n",
      "82484          False  False\n",
      "82485          False  False\n",
      "\n",
      "[82486 rows x 2 columns]\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "isnull().sum()\n",
      "text_combined    0\n",
      "label            0\n",
      "dtype: int64\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "isnull().values.any()\n",
      "text_combined    0\n",
      "label            0\n",
      "dtype: int64\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "eq().values.any()\n",
      "False\n",
      "---------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------\n",
      "Check which columns have NaN values\n",
      "---------------------------------------------------------\n",
      "\n",
      " Test Run Ok 2.3\n"
     ]
    }
   ],
   "source": [
    " # Upload file.csv from local computer\n",
    "df = pd.read_csv(DATA_PATH, encoding='utf-8', low_memory=False)\n",
    "\n",
    "print('df')\n",
    "print(' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ')\n",
    "print('\\n')\n",
    "\n",
    "print('head')\n",
    "df.head()\n",
    "print( df.head())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('columns names')\n",
    "print(df.columns)\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('Count of columns :', len(df.columns))\n",
    "print('Count of rows :  ', len(df))\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('infor')\n",
    "print(df.info())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print(' tail ')\n",
    "df.tail()\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('shape')\n",
    "print(df.shape)\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "print('length of data is', len(df))\n",
    "\n",
    "print('types')\n",
    "print(df.dtypes)\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('describe')\n",
    "print(df.describe())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('isnull()')\n",
    "print(df.isnull())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "#Use Pandas to explore and clean up your tabular data\n",
    "print('isnull().sum()')\n",
    "print(df.isnull().sum())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "# Python, check if there are missing cells\n",
    "\n",
    "print('isnull().values.any()')\n",
    "print(df.isnull().sum())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('eq('').values.any()')\n",
    "print(df.eq('').values.any())\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('Check which columns have NaN values')\n",
    "# Check which columns have NaN values\n",
    "columns_with_nan = df.isnull().any()\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('')\n",
    "# Get actual column names from the DataFrame\n",
    "columns_to_count = df.columns\n",
    "\n",
    "# Now you can use value_counts on all columns\n",
    "df[columns_to_count].value_counts()\n",
    "\n",
    "print(' Test Run Ok 2.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad588cf-1e5c-44b3-8fd7-51df165cfc46",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "convert everything to lowercase strings, Since the processor and compiler can be context-sensitive, all columns are converted to lower case\n",
    "Check Category's values.\n",
    "Category [legit, phishing] in lowercase format\n",
    "To the words 'phishing', 'phish', 'spam', 'scam' they are given format 'phishing\n",
    "## Numeric target\n",
    "'y' = map({'legit': 0, 'phishing': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d0660b-7fce-4c33-a334-b72db0a176fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legit: 0\n",
      " phishing: 1\n",
      "Label counts:\n",
      " y\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n",
      " Test 3.1 Run OK -\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Clean labels robustly\n",
    "# -------------------------\n",
    "\n",
    "# convert everything to lowercase strings\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# numeric datasets (like yours)\n",
    "df['label'] = df['label'].replace({\n",
    "    '0': 'legit',\n",
    "    '1': 'phishing'\n",
    "})\n",
    "\n",
    "# map typical variants\n",
    "label_map = {\n",
    "    'phishing': 'phishing',\n",
    "    'phish': 'phishing',\n",
    "    'spam': 'phishing',\n",
    "    'scam': 'phishing',\n",
    "    'ham': 'legit',\n",
    "    'legit': 'legit',\n",
    "    'legitimate': 'legit',\n",
    "    'benign': 'legit'\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].apply(lambda s: label_map.get(s, 'unknown'))\n",
    "\n",
    "# keep only valid rows\n",
    "df = df[df['label'].isin(['phishing','legit'])].copy()\n",
    "\n",
    "# numeric target\n",
    "df['y'] = df['label'].map({'legit': 0, 'phishing': 1})\n",
    "\n",
    "print(\"legit: 0\\n phishing: 1\")\n",
    "print(\"Label counts:\\n\", df['y'].value_counts())\n",
    "print(' Test 3.1 Run OK -')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb5bf3-107d-494a-9336-5cf572f1ea35",
   "metadata": {},
   "source": [
    " ## Text preprocessing\n",
    " Text messages are being refined.\n",
    " Some words are removed from the stopwords; since their presence can affect the detection of phishing messages.\n",
    " Unnecessary characters that do not affect the semantics of the text are removed.\n",
    " The URLs are removed\n",
    " Messages are tokenized basic units and lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebeedac4-a21b-4164-ad48-238f8017cc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test 3.2 Run OK -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wn = WordNetLemmatizer()\n",
    "builtin_stop = set(stopwords.words('english'))\n",
    "\n",
    "# remove these from stopword removal because they're useful for phishing detection\n",
    "keep_tokens = {'account','password','verify','suspend','suspended','login','bank','click','link','update','confirm','security'}\n",
    "# final stoplist = builtin_stop minus keep_tokens\n",
    "stoplist = builtin_stop - keep_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): \n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # optional: normalize unicode\n",
    "    text = re.sub(r'https?://\\S+',' ', text)   # remove URLs (or keep them if you want to extract domain features)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # keep alphanumerics (numbers can help)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stoplist]\n",
    "    tokens = [wn.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text_combined'].apply(preprocess_text)\n",
    "#print(df)\n",
    "print(' Test 3.2 Run OK -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88044b7-d522-4153-a378-64a84afe17fc",
   "metadata": {},
   "source": [
    "##  Balance classes \n",
    "In machine learning models, in many cases, if the models are trained with unbalanced data, there is a tendency to predict the one with the larger number.\n",
    "\n",
    "legit: 0\n",
    "phishing: 1\n",
    "Label counts:\n",
    "0 39595\n",
    "1 42891\n",
    "\n",
    "Since the phishing class (1) is smaller than the legit class (0), a balance is created.\n",
    "The following code snippet checks if there is an imbalance. Then, since legitimate labels are in the minority, they are balanced with phishing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656f7de4-8ebb-42f6-8f4d-75efe8e9171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing: y\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n",
      "legit: 0\n",
      " phishing: 1\n",
      "After balancing: y\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n",
      " Test 3.3 Run OK -\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 4) Balance classes (upsample phishing if minority)\n",
    "# -------------------------\n",
    "print(\"Before balancing:\", df['y'].value_counts())\n",
    "n_phish = df['y'].sum()\n",
    "n_legit = len(df) - n_phish\n",
    "\n",
    "if n_phish == 0:\n",
    "    raise RuntimeError(\"No phishing samples found — add phishing data!\")\n",
    "\n",
    "if n_phish < n_legit:\n",
    "    phishing = df[df['y']==1]\n",
    "    legit = df[df['y']==0]\n",
    "    phishing_upsampled = resample(phishing, replace=True, n_samples=len(legit), random_state=42)\n",
    "    df_bal = pd.concat([legit, phishing_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_bal = df.copy()\n",
    "\n",
    "print(\"legit: 0\\n phishing: 1\")\n",
    "print(\"After balancing:\", df_bal['y'].value_counts())\n",
    "print(' Test 3.3 Run OK -')\n",
    "# -------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2167e7-69da-4f9c-8e02-926487921c26",
   "metadata": {},
   "source": [
    "## Train/test split and TF-IDF\n",
    "## Selecting the input (X) and target (y)\n",
    " Splitting into train/test sets\n",
    " 75% training and 25% testing.\n",
    " stratify=y ensures both train and test have the same class proportions.\n",
    " random_state=42 ensures reproducibility.\n",
    " They are vectorized using NGram\n",
    "TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "Assigns numerical characteristics to text messages so machine learning can better process the data.\n",
    "\n",
    "TF: What is the frequency of a word?\n",
    "IDF: How rare is the word?\n",
    "\n",
    "N-grams: These are important because phrases, not just words, are used in message semantics.\n",
    "\n",
    "Example:\n",
    "ngram_range=(1,3)\n",
    "Unigram: \"update\"\n",
    "Bigram: \"update account\"\n",
    "Trigram: \"verify your account\"\n",
    "\n",
    "lowercase=True: Converts all text to lowercase.\n",
    "max_features=50000: Preserves the 50,000 most important features.\n",
    "\n",
    "tfidf.transform(X_test): Transforms the test data using the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21aaf89f-50be-4324-b8d6-168f8aefd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test 3.4 Run OK -\n"
     ]
    }
   ],
   "source": [
    "# 5) Train/test split and TF-IDF\n",
    "# -------------------------\n",
    "X = df_bal['clean_text']\n",
    "y = df_bal['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "# TF-IDF: use 1-3 grams (unigrams, bigrams, trigrams)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), lowercase=True, max_features=50000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "print(' Test 3.4 Run OK -')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58abbe6-64b7-4b97-ad4c-ddd4e03876d8",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c269ed-3f30-4023-a31a-055b91df8851",
   "metadata": {},
   "source": [
    "## The ML's mathematical algorithms are implemented.\n",
    "## The mathematical models are evaluated.\n",
    "## Logistic Regression\n",
    "## max_iter=2000 → give the model enough iterations to converge\n",
    "## class_weight='balanced' → correct class imbalance\n",
    "## solver='liblinear' → reliable solver for small/medium datasets and binary classification\n",
    "\n",
    "## Naive Bayes\n",
    "## Simple and fast text classifier\n",
    "## Works well with TF-IDF or CountVectorizer\n",
    "\n",
    "## Train Each Model and Evaluate It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dfbf6-5233-4e7c-bcb7-1ccffa188468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6) Train models and evaluate\n",
    "# -------------------------\n",
    "'''\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear'),\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "    'SVC': SVC(probability=True, class_weight='balanced', kernel='linear'),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "} \n",
    "'''\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear'),\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train_tfidf, y_train)\n",
    "    preds = m.predict(X_test_tfidf)\n",
    "    print(f\"\\n=== {name} ==============================\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    #print(classification_report(y_test, preds, digits=4))\n",
    "    # cm = confusion_matrix(y_test, preds)\n",
    "    # print(\"Confusion matrix:\\n\", cm)\n",
    "    # results[name] = m\n",
    "    confusion_matrixs=confusion_matrix(y_test,preds)\n",
    "    sns.heatmap(confusion_matrixs,annot=True,fmt=\"1.0f\",cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f\"Confusion Matrix For {name}\")\n",
    "    plt.show()\n",
    "    #print('\\n')\n",
    "      \n",
    "    # Extract values\n",
    "    TN, FP, FN, TP = confusion_matrixs.ravel()\n",
    "    \n",
    "    #print(\"\")\n",
    "    print(f\"True Negative (TN) = {TN}\")\n",
    "    print(f\"False Positive (FP) = {FP}\")\n",
    "    print(f\"False Negative (FN) = {FN}\")\n",
    "    print(f\"True Positive (TP) = {TP}\")\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    print(\"Accuracy :\" , accuracy)\n",
    "    print('\\n')\n",
    "    #print(' \\n Run Ok 1') \n",
    "    classification_reports=classification_report(y_test,preds)\n",
    "    print(f'{name} classification_report:\\n{classification_reports}')\n",
    "    #print('\\n')\n",
    "    \n",
    "print(' Test 4 Run OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f6d9e-a00a-4f57-8476-48b99eec8511",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52ba28-2c3c-42ad-9862-e17b46f5c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose best by F1 or accuracy (example use LogisticRegression)\n",
    "best_name = max(models.keys(), key=lambda n: accuracy_score(y_test, models[n].predict(X_test_tfidf)))\n",
    "best_model = models[best_name]\n",
    "print(\"Best:\", best_name)\n",
    "\n",
    "# Save vectorizer and model\n",
    "joblib.dump(tfidf, VECT_OUT)\n",
    "joblib.dump(best_model, MODEL_OUT)\n",
    "print(\"Saved:\", VECT_OUT, MODEL_OUT)\n",
    "print(' Test 5 Run OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527bc5a-fa44-449a-a925-57a2fb438c1d",
   "metadata": {},
   "source": [
    "# 6. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41e17865-631f-4cee-9fa7-b9f1075415d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test 3 Run OK\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7) Prediction helper\n",
    "# -------------------------\n",
    "def predict_email(text, model=best_model, vectorizer=tfidf):\n",
    "    cleaned = preprocess_text(text)\n",
    "    vector = vectorizer.transform([cleaned])\n",
    "    pred = model.predict(vector)[0]\n",
    "    prob = model.predict_proba(vector)[0] if hasattr(model, \"predict_proba\") else None\n",
    "    return pred, prob\n",
    "'''\n",
    "# quick test\n",
    "msg = \"Your account will be suspended, verify your password now.\"\n",
    "pred, prob = predict_email(msg)\n",
    "label_map = {0: \"legit\", 1: \"phishing\"}\n",
    "print(\"Prediction:\", label_map[pred])\n",
    "print(\"Probabilities:\", prob)\n",
    "'''\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def run_gui():\n",
    "    def diagnose():\n",
    "        text = text_entry.get(\"1.0\", tk.END).strip()\n",
    "        pred, prob = predict_email(text)\n",
    "        label_map = {0: \"legit\", 1: \"phishing\"}\n",
    "        label = label_map[pred]\n",
    "        probability = prob[pred]\n",
    "\n",
    "        messagebox.showinfo(\"Diagnosis\", f\"{label.upper()} (prob: {probability:.4f})\")\n",
    "\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Phishing Message Detector\")\n",
    "\n",
    "    tk.Label(window, text=\"Enter message:\").pack()\n",
    "\n",
    "    text_entry = tk.Text(window, height=6, width=60)\n",
    "    text_entry.pack()\n",
    "\n",
    "    tk.Button(window, text=\"Check\", command=diagnose).pack()\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "run_gui()\n",
    "\n",
    "\n",
    "print(' Test 6 Run OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec168a-4c0b-4e3e-8b45-2c56af12692e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d225e-9cb5-45a4-b182-4451c9fa9d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
